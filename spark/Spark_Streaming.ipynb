{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Data from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, BooleanType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/envs/data_eng/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/bharathvelamala/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/bharathvelamala/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-06f90e5c-8985-46d0-974b-f3bd8e2abe38;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.4 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.4 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 323ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.4 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.4 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-06f90e5c-8985-46d0-974b-f3bd8e2abe38\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/6ms)\n",
      "25/03/09 00:31:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaSparkConsumer1\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/kafka_checkpoint\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_schema = StructType() \\\n",
    "    .add(\"channel_id\", StringType()) \\\n",
    "    .add(\"title\", StringType()) \\\n",
    "    .add(\"description\", StringType()) \\\n",
    "    .add(\"custom_url\", StringType()) \\\n",
    "    .add(\"published_at\", StringType()) \\\n",
    "    .add(\"country\", StringType()) \\\n",
    "    .add(\"subscriber_count\", IntegerType()) \\\n",
    "    .add(\"view_count\", IntegerType()) \\\n",
    "    .add(\"video_count\", IntegerType()) \\\n",
    "    .add(\"hidden_subscriber_count\", BooleanType()) \\\n",
    "    .add(\"high_thumbnail\", StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_schema = StructType() \\\n",
    "    .add(\"video_id\", StringType()) \\\n",
    "    .add(\"title\", StringType()) \\\n",
    "    .add(\"description\", StringType()) \\\n",
    "    .add(\"description_summary\", StringType()) \\\n",
    "    .add(\"channel_id\", StringType()) \\\n",
    "    .add(\"channel_title\", StringType()) \\\n",
    "    .add(\"published_at\", StringType()) \\\n",
    "    .add(\"published_year\", StringType()) \\\n",
    "    .add(\"view_count\", IntegerType()) \\\n",
    "    .add(\"like_count\", IntegerType()) \\\n",
    "    .add(\"comment_count\", IntegerType()) \\\n",
    "    .add(\"favorite_count\", IntegerType()) \\\n",
    "    .add(\"engagement_ratio\", IntegerType()) \\\n",
    "    .add(\"likes_per_view\", IntegerType()) \\\n",
    "    .add(\"comments_per_view\", IntegerType()) \\\n",
    "    .add(\"thumbnail_url\", StringType()) \\\n",
    "    .add(\"thumbnail_width\", IntegerType()) \\\n",
    "    .add(\"thumbnail_height\", IntegerType()) \\\n",
    "    .add(\"duration\", StringType()) \\\n",
    "    .add(\"definition\", StringType()) \\\n",
    "    .add(\"caption\", BooleanType()) \\\n",
    "    .add(\"licensed_content\", BooleanType()) \\\n",
    "    .add(\"tags\", StringType()) \\\n",
    "    .add(\"tag_count\", IntegerType()) \\\n",
    "    .add(\"category_id\", StringType()) \\\n",
    "    .add(\"live_broadcast_content\", StringType()) \\\n",
    "    .add(\"default_language\", StringType()) \\\n",
    "    .add(\"default_audio_language\", StringType()) \\\n",
    "    .add(\"privacy_status\", StringType()) \\\n",
    "    .add(\"upload_status\", StringType()) \\\n",
    "    .add(\"embeddable\", BooleanType()) \\\n",
    "    .add(\"made_for_kids\", BooleanType()) \\\n",
    "    .add(\"title_length\", IntegerType()) \\\n",
    "    .add(\"description_length\", IntegerType()) \\\n",
    "    .add(\"has_hashtags\", BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_schema = StructType() \\\n",
    "    .add(\"comment_id\", StringType()) \\\n",
    "    .add(\"video_id\", StringType()) \\\n",
    "    .add(\"author\", StringType()) \\\n",
    "    .add(\"content\", StringType()) \\\n",
    "    .add(\"published_at\", StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_schema = StructType() \\\n",
    "    .add(\"video_id\", StringType()) \\\n",
    "    .add(\"caption_id\", StringType()) \\\n",
    "    .add(\"language\", StringType()) \\\n",
    "    .add(\"caption_text\", StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_video_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"youtube_video_info\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "kafka_channel_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"youtube_channel_info\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "kafka_comment_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"youtube_video_comments\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "kafka_captions_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"youtube_video_captions\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/09 00:31:24 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "video_parsed_df = kafka_video_df \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), video_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "channel_parsed_df = kafka_channel_df \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), channel_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "comment_parsed_df = kafka_comment_df \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), comment_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "captions_parsed_df = kafka_captions_df \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), captions_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- description_summary: string (nullable = true)\n",
      " |-- channel_id: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- published_at: string (nullable = true)\n",
      " |-- published_year: string (nullable = true)\n",
      " |-- view_count: integer (nullable = true)\n",
      " |-- like_count: integer (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- favorite_count: integer (nullable = true)\n",
      " |-- engagement_ratio: integer (nullable = true)\n",
      " |-- likes_per_view: integer (nullable = true)\n",
      " |-- comments_per_view: integer (nullable = true)\n",
      " |-- thumbnail_url: string (nullable = true)\n",
      " |-- thumbnail_width: integer (nullable = true)\n",
      " |-- thumbnail_height: integer (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- definition: string (nullable = true)\n",
      " |-- caption: boolean (nullable = true)\n",
      " |-- licensed_content: boolean (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- tag_count: integer (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- live_broadcast_content: string (nullable = true)\n",
      " |-- default_language: string (nullable = true)\n",
      " |-- default_audio_language: string (nullable = true)\n",
      " |-- privacy_status: string (nullable = true)\n",
      " |-- upload_status: string (nullable = true)\n",
      " |-- embeddable: boolean (nullable = true)\n",
      " |-- made_for_kids: boolean (nullable = true)\n",
      " |-- title_length: integer (nullable = true)\n",
      " |-- description_length: integer (nullable = true)\n",
      " |-- has_hashtags: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "video_parsed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- channel_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- custom_url: string (nullable = true)\n",
      " |-- published_at: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- subscriber_count: integer (nullable = true)\n",
      " |-- view_count: integer (nullable = true)\n",
      " |-- video_count: integer (nullable = true)\n",
      " |-- hidden_subscriber_count: boolean (nullable = true)\n",
      " |-- high_thumbnail: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "channel_parsed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- comment_id: string (nullable = true)\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- published_at: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comment_parsed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- caption_id: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- caption_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "captions_parsed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/09 00:31:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/03/09 00:31:41 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+----------+----------+-------------+--------------+----------------+--------------+-----------------+--------------------+---------------+----------------+--------+----------+-------+----------------+--------------------+---------+-----------+----------------------+----------------+----------------------+--------------+-------------+----------+-------------+------------+------------------+------------+\n",
      "|   video_id|               title|         description| description_summary|          channel_id|       channel_title|        published_at|published_year|view_count|like_count|comment_count|favorite_count|engagement_ratio|likes_per_view|comments_per_view|       thumbnail_url|thumbnail_width|thumbnail_height|duration|definition|caption|licensed_content|                tags|tag_count|category_id|live_broadcast_content|default_language|default_audio_language|privacy_status|upload_status|embeddable|made_for_kids|title_length|description_length|has_hashtags|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+----------+----------+-------------+--------------+----------------+--------------+-----------------+--------------------+---------------+----------------+--------+----------+-------+----------------+--------------------+---------+-----------+----------------------+----------------+----------------------+--------------+-------------+----------+-------------+------------+------------------+------------+\n",
      "|rfscVS0vtbw|Learn Python - Fu...|This course will ...|This course will ...|UC8butISFwT-Wl7EV...|    freeCodeCamp.org|2018-07-11T18:00:42Z|          2018|  46163397|   1072515|        44993|             0|            NULL|          NULL|             NULL|https://i.ytimg.c...|           1280|             720|        |          |  false|           false|[\"python\",\"python...|       14|         27|                  none|              en|                    en|              |             |      true|        false|          51|              3739|        true|\n",
      "|HeQX2HjkcNo|Math's Fundamenta...|Not everything th...|Not everything th...|UCHnyfMqiRRG1u-2M...|          Veritasium|2021-05-22T14:23:50Z|          2021|  28603310|    742611|        49794|             0|            NULL|          NULL|             NULL|https://i.ytimg.c...|           1280|             720|        |          |  false|           false|[\"veritasium\",\"sc...|        3|         27|                  none|              en|                    en|              |             |      true|        false|          23|              3230|       false|\n",
      "|PmlRbfSavbI|Stealing Baseball...|I always sucked a...|I always sucked a...|UCY1kMZp36IQSyNx_...|          Mark Rober|2019-06-30T17:23:23Z|          2019|  28464186|    453906|        14052|             0|            NULL|          NULL|             NULL|https://i.ytimg.c...|           1280|             720|        |          |  false|           false|[\"mark rober\",\"ba...|       19|         28|                  none|              en|                    en|              |             |      true|        false|          55|              1744|        true|\n",
      "|UT2noVDFoaA|Help Protect the ...|We are excited to...|We are excited to...|UC0rqucBdTuFTjJie...|          TensorFlow|2021-11-22T20:01:21Z|          2021|  26013159|      1590|           37|             0|            NULL|          NULL|             NULL|https://i.ytimg.c...|           1280|             720|        |          |  false|           false|[\"Machine Learnin...|       11|         28|                  none|           en-US|                    en|              |             |      true|        false|          57|               412|       false|\n",
      "|49MwTFZV7Z4|Pinky and Panda F...|Pinky and Panda F...|Pinky and Panda F...|UCVZ6IwlLkrP6tFiO...|Pinky and Panda T...|2018-04-18T05:59:07Z|          2018|  22885690|         0|            0|             0|            NULL|          NULL|             NULL|https://i.ytimg.c...|           1280|             720|        |          |  false|           false|[\"fun play\",\"Toy ...|        8|         27|                  none|                |                      |              |             |      true|        false|          71|                71|       false|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+----------+----------+-------------+--------------+----------------+--------------+-----------------+--------------------+---------------+----------------+--------+----------+-------+----------------+--------------------+---------+-----------+----------------------+----------------+----------------------+--------------+-------------+----------+-------------+------------+------------------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_query = video_parsed_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Await termination for 10 seconds\n",
    "video_query.awaitTermination(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/09 00:31:51 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/03/09 00:31:51 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-------+----------------+----------+-----------+-----------------------+--------------------+\n",
      "|          channel_id|               title|         description|          custom_url|        published_at|country|subscriber_count|view_count|video_count|hidden_subscriber_count|      high_thumbnail|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-------+----------------+----------+-----------+-----------------------+--------------------+\n",
      "|UCY1kMZp36IQSyNx_...|          Mark Rober|Former NASA engin...|          @markrober|2011-10-20T06:17:58Z|     US|        65000000|      NULL|        199|                  false|https://yt3.ggpht...|\n",
      "|UCHnyfMqiRRG1u-2M...|          Veritasium|An element of tru...|         @veritasium|2010-07-21T07:18:02Z|     US|        17400000|      NULL|        431|                  false|https://yt3.ggpht...|\n",
      "|UC8butISFwT-Wl7EV...|    freeCodeCamp.org|Learn to code for...|       @freecodecamp|2014-12-16T21:18:48Z|     US|        10500000| 839961366|       1820|                  false|https://yt3.ggpht...|\n",
      "|UCVZ6IwlLkrP6tFiO...|Pinky and Panda T...|KIDS will Enjoy t...|@pinkyandpandatoystv|2016-12-18T11:28:00Z|     US|         1590000|1064176310|        214|                  false|https://yt3.ggpht...|\n",
      "|UC0rqucBdTuFTjJie...|          TensorFlow|Welcome to the of...|         @tensorflow|2017-12-22T18:07:53Z|    N/A|          612000| 123012194|        673|                  false|https://yt3.ggpht...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-------+----------------+----------+-----------+-----------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "channel_query = channel_parsed_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "channel_query.awaitTermination(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/09 00:32:01 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/03/09 00:32:01 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+-----------+------+-------+--------------------+\n",
      "|          comment_id|   video_id|author|content|        published_at|\n",
      "+--------------------+-----------+------+-------+--------------------+\n",
      "|Ugxqcek3RUzxWv-4V...|rfscVS0vtbw|  NULL|   NULL|2021-03-09T18:41:00Z|\n",
      "|Ugwq4YOvmRyQ2fsLw...|rfscVS0vtbw|  NULL|   NULL|2025-03-08T20:43:55Z|\n",
      "|UgwVFMfeiPB31-qw2...|rfscVS0vtbw|  NULL|   NULL|2025-03-08T17:51:36Z|\n",
      "|Ugzawl_y07yvoVdXG...|rfscVS0vtbw|  NULL|   NULL|2025-03-08T17:12:55Z|\n",
      "|Ugz7QegE71kp7xEFe...|rfscVS0vtbw|  NULL|   NULL|2025-03-07T12:56:53Z|\n",
      "|Ugz2N2o8Ng9dxWCkc...|HeQX2HjkcNo|  NULL|   NULL|2025-03-08T20:21:54Z|\n",
      "|Ugxr_KxzAi9S1Pea5...|HeQX2HjkcNo|  NULL|   NULL|2025-03-08T18:13:12Z|\n",
      "|UgxCz9B4OURllu82-...|HeQX2HjkcNo|  NULL|   NULL|2025-03-08T00:25:02Z|\n",
      "|Ugz64UrqfYiZAEYxd...|HeQX2HjkcNo|  NULL|   NULL|2025-03-07T22:31:47Z|\n",
      "|UgwZyfVDT6rVGiqZW...|HeQX2HjkcNo|  NULL|   NULL|2025-03-07T22:29:03Z|\n",
      "|UgyP-UhQUp1EiNf7p...|PmlRbfSavbI|  NULL|   NULL|2025-02-22T12:46:31Z|\n",
      "|UgxqJbEoT076tJ18J...|PmlRbfSavbI|  NULL|   NULL|2025-02-21T20:09:22Z|\n",
      "|Ugy5UBc6Oqt5AB0C1...|PmlRbfSavbI|  NULL|   NULL|2025-02-14T08:10:11Z|\n",
      "|UgzA8frlz5kHFoUUz...|PmlRbfSavbI|  NULL|   NULL|2024-12-17T03:24:42Z|\n",
      "|UgxK2Tl5c-XeqNfdi...|PmlRbfSavbI|  NULL|   NULL|2024-12-08T11:27:09Z|\n",
      "|Ugx0DAU_W1GIOcnDm...|UT2noVDFoaA|  NULL|   NULL|2024-08-01T14:25:52Z|\n",
      "|UgyLsnpH0RcrbE3E0...|UT2noVDFoaA|  NULL|   NULL|2023-09-28T00:03:25Z|\n",
      "|Ugzg636Xu_PEBMjQQ...|UT2noVDFoaA|  NULL|   NULL|2023-03-24T06:20:44Z|\n",
      "|Ugw7sldloB3ay_td-...|UT2noVDFoaA|  NULL|   NULL|2023-02-25T08:11:17Z|\n",
      "|UgyrD3WhovXsnQKET...|UT2noVDFoaA|  NULL|   NULL|2023-01-26T00:58:48Z|\n",
      "+--------------------+-----------+------+-------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_query = comment_parsed_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "comment_query.awaitTermination(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/09 00:32:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/03/09 00:32:11 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----------+--------------------+--------+------------+\n",
      "|   video_id|          caption_id|language|caption_text|\n",
      "+-----------+--------------------+--------+------------+\n",
      "|rfscVS0vtbw|AUieDaZvswHvw8s1O...|      es|        NULL|\n",
      "|rfscVS0vtbw|AUieDaalEVHfBYqan...|      ro|        NULL|\n",
      "|rfscVS0vtbw|AUieDaafT99UerDMT...|      hi|        NULL|\n",
      "|rfscVS0vtbw|AUieDaZMHyzB52Qby...|   zh-CN|        NULL|\n",
      "|rfscVS0vtbw|AUieDaYrzqWNIrQUq...|      bg|        NULL|\n",
      "|rfscVS0vtbw|AUieDabvb8bdpB5FA...|      id|        NULL|\n",
      "|rfscVS0vtbw|AUieDaZRdp8cgoDta...|      pl|        NULL|\n",
      "|rfscVS0vtbw|AUieDabmSOqF0gefj...|      pt|        NULL|\n",
      "|rfscVS0vtbw|AUieDaY01nROBDzz-...|      ru|        NULL|\n",
      "|rfscVS0vtbw|AUieDaZx0-GUIUgil...|      ko|        NULL|\n",
      "|rfscVS0vtbw|AUieDaYbLR4VDZKJI...|      ar|        NULL|\n",
      "|rfscVS0vtbw|AUieDabnPBrrcphB9...|      en|        NULL|\n",
      "|rfscVS0vtbw|AUieDabnU-QoWqFDg...|      iw|        NULL|\n",
      "|rfscVS0vtbw|AUieDaYoT-e_vQ-cC...|      tr|        NULL|\n",
      "|HeQX2HjkcNo|AUieDaaVAOCYm29UE...|      en|        NULL|\n",
      "|HeQX2HjkcNo|AUieDabvxvpahLBnZ...|      en|        NULL|\n",
      "|HeQX2HjkcNo|AUieDaaAvr77UesBr...|      zh|        NULL|\n",
      "|PmlRbfSavbI|AUieDabrmPSdrJpEa...|      ru|        NULL|\n",
      "|PmlRbfSavbI|AUieDaatxHmQozi9G...|   pt-BR|        NULL|\n",
      "|PmlRbfSavbI|AUieDabwQJGyCjlYw...|      tr|        NULL|\n",
      "+-----------+--------------------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_query = captions_parsed_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "captions_query.awaitTermination(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
