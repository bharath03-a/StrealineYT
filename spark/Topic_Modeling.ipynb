{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pyspark pandas numpy scikit-learn pyLDAvis gensim mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load Transcripts Data from Kafka Stream (PySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, IntegerType\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"YouTubeTranscriptAnalysis\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/kafka_checkpoint\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define transcript schema\n",
    "transcripts_schema = StructType([\n",
    "    StructField(\"videoId\", StringType(), True),\n",
    "    StructField(\"transcript\", ArrayType(StructType([\n",
    "        StructField(\"text\", StringType(), True),\n",
    "        StructField(\"start\", FloatType(), True),\n",
    "        StructField(\"duration\", FloatType(), True)\n",
    "    ])), True)\n",
    "])\n",
    "\n",
    "# Read Kafka Stream\n",
    "kafka_transcripts_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"youtube_transcripts\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON data\n",
    "transcripts_parsed_df = kafka_transcripts_df \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), transcripts_schema).alias(\"data\")) \\\n",
    "    .select(\"data.videoId\", \"data.transcript\")\n",
    "\n",
    "# Flatten transcripts (convert list of text into a single document per video)\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def concatenate_transcript(transcript):\n",
    "    if transcript is not None:\n",
    "        return \" \".join([t[\"text\"] for t in transcript if t[\"text\"] is not None])\n",
    "    return \"\"\n",
    "\n",
    "concat_udf = udf(concatenate_transcript, StringType())\n",
    "transcripts_parsed_df = transcripts_parsed_df.withColumn(\"full_transcript\", concat_udf(col(\"transcript\"))).select(\"videoId\", \"full_transcript\")\n",
    "\n",
    "# Convert to Pandas\n",
    "transcripts_pd = transcripts_parsed_df.toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Preprocess Transcript Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim import corpora, models\n",
    "import numpy as np\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Text cleaning function\n",
    "def preprocess_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"\\W\", \" \", text.lower())  # Remove special characters\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 2]\n",
    "    return \" \".join(words)\n",
    "\n",
    "transcripts_pd[\"cleaned_transcript\"] = transcripts_pd[\"full_transcript\"].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Apply Topic Modeling (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Convert text into a document-term matrix\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=\"english\")\n",
    "transcript_matrix = vectorizer.fit_transform(transcripts_pd[\"cleaned_transcript\"])\n",
    "\n",
    "# Train LDA model\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)  # Adjust topics as needed\n",
    "lda.fit(transcript_matrix)\n",
    "\n",
    "# Get topic words\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "topic_words = {}\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    topic_words[topic_idx] = [terms[i] for i in topic.argsort()[:-10 - 1:-1]]  # Top 10 words\n",
    "\n",
    "# Print Topics\n",
    "for topic, words in topic_words.items():\n",
    "    print(f\"Topic {topic}: {', '.join(words)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Visualize Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "visualization = pyLDAvis.sklearn.prepare(lda, transcript_matrix, vectorizer)\n",
    "pyLDAvis.display(visualization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Track with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "mlflow.set_experiment(\"YouTube Topic Modeling\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"n_topics\", 5)\n",
    "    mlflow.sklearn.log_model(lda, \"LDA_model\")\n",
    "    mlflow.log_artifact(\"lda_visualization.html\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
