- After testing these function run these as airflow DAGS
- Then create Kafka Topics, and configure producers and consumer adn try to pushh to Kafka

- If everything is done create a workflow as to what is needed here?
- Then try to setup hive over here and sink the raw data to hive db too...
- Setup Spark Streaming or Kafka Streaming
- Then perform some Data transformations - understanding the data or some ML stuff?
- NLP stuff can be done - like understanding the comments


airflow:
    docker-compose up airflow-init
    docker-compose up -d
    http://localhost:8080

    docker-compose down -v
docker-compose up --build


docker-compose up -d --no-deps --build airflow-webserver airflow-scheduler

docker build . --tag extending_airflow:latest

issue with pydantic validation LoL - setup .env correctly enoughz

installing the libraries are necessary or else we will have import DAG error

-- LEARN AIRFLOW
-- KAFKA SETUP(confluent)
-- APACHE HIVE(how) - signed up for qubolt but not sure if I can get anything from them